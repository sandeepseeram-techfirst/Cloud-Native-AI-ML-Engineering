# 🌩️ Cloud Native AI – Concepts, Tools & Practical Guide

## 📌 Overview
**Cloud Native AI** is the practice of building, deploying, and managing AI/ML workloads using **cloud native principles** — scalability, resilience, automation, and portability — typically powered by **Kubernetes** and container-based architectures.

This repository is my learning journal, experiments, and implementations for **Cloud Native AI**.

---

## 🚀 What is Cloud Native AI?

Cloud Native AI combines:
- **AI/ML** → Training, inference, and serving of machine learning and deep learning models.
- **Cloud Native** → Containers, Kubernetes, CI/CD, observability, and scalability across hybrid/multi-cloud environments.

**Why it matters:**
- **Elastic compute** → Scale GPU/TPU workloads up/down on demand.
- **Portable** → Run across AWS, GCP, Azure, or on-prem Kubernetes.
- **Automated** → Continuous training, testing, and deployment pipelines.
- **Observable** → Track infrastructure, model performance, and drift.

---

```
┌─────────────────────────────────────────────────────┐
│                  Cloud Native AI Stack              │
├─────────────────────────────────────────────────────┤
│ Infra: Kubernetes, GPU Nodes, Knative               │
│ AI DevOps: Kubeflow, MLflow, Feast, Airflow, Ray    │
│ Serving: KServe, Seldon, NVIDIA Triton, ONNX Runtime│
│ Data: Spark, BigQuery, Delta Lake                   │
│ Observability: Prometheus, Grafana, Evidently AI    │
└─────────────────────────────────────────────────────┘
```
